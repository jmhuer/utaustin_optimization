{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "e-greedy-assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/utaustin_optimization/blob/main/homework8/e_greedy_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg6j20e4VJDm"
      },
      "source": [
        "# Epsilon Greedy\n",
        "\n",
        "In this excercises, we will be playing with the Multi-arm bandit problem with the Epsilon Greedy algorithm.\n",
        "\n",
        "## Setup\n",
        "\n",
        "As before, consider unstructural bandit problem. Suppose we have $k$ arms, each with random rewards $p_i = u_i + \\epsilon$ where $\\epsilon$ is draw from i.i.d. standard gaussian. (Note that we only require $\\epsilon$ to be sub-gaussian for the analysis to go through)\n",
        "\n",
        "The following codes is capturing the setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVZB0Qo2VJjv"
      },
      "source": [
        "import numpy as np\n",
        "import pdb\n",
        "\n",
        "\n",
        "class Gaussian_Arm:\n",
        "  def __init__(self, num_arms, mu=None):\n",
        "    '''\n",
        "    num_arms: (int). the number of arms\n",
        "    mu: (None or list-type). the mean of the reward of each arm.\n",
        "        if set to None, a random vector will be generated.\n",
        "    '''\n",
        "    if num_arms <= 1 or not isinstance(num_arms, int):\n",
        "      print('number of arms has an int that is at least two')\n",
        "      return\n",
        "    \n",
        "    self.num_arms = num_arms\n",
        "    #\n",
        "    if mu:\n",
        "      self.mu = np.asarray(mu)\n",
        "      if len(self.mu) != num_arms:\n",
        "        print('The lenth of mu does not match the number of arms')\n",
        "        return\n",
        "    else:\n",
        "      self.mu = np.random.rand((num_arms))\n",
        "    # \n",
        "    self.delta = max(self.mu) - min(self.mu)\n",
        "    #\n",
        "\n",
        "    # keep track of the rewards for the user\n",
        "    self.rewards_history = []\n",
        "    # keep track of how many times the arms have been pulled\n",
        "    self.total_pull = 0 \n",
        "\n",
        "  def pull_arm(self, arm_id=-1):\n",
        "    if arm_id < 0 or arm_id >= self.num_arms:\n",
        "      print('please specify arm id in the range of 0-%d' % (self.num_arms))\n",
        "      return\n",
        "    self.total_pull += 1\n",
        "    # Generate reward\n",
        "    reward = self.mu[arm_id] + np.random.randn()\n",
        "    self.rewards_history.append(reward)\n",
        "    return reward\n",
        "\n",
        "\n",
        "  def genie_reward(self):\n",
        "    '''\n",
        "    the best expected reward after pulling self.total_pull times\n",
        "    '''\n",
        "    best_mu = max(self.mu)\n",
        "    return self.total_pull * best_mu\n",
        "\n",
        "  def my_rewards(self):\n",
        "    return sum(self.rewards_history)\n",
        "\n",
        "  def clear_reward_hist(self):\n",
        "    self.rewards_history = []\n",
        "    self.total_pull = 0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yLTWMxeVFFO"
      },
      "source": [
        "## Goal\n",
        "\n",
        "Implement the following:\n",
        "\n",
        "1. Basic epsilon greedy algorithm implementation\n",
        "2. Plot the expected regret of the epsilong greedy algorithm VS horizon ($n$).\n",
        "\n",
        "## Tips:\n",
        "\n",
        "1. The regret is expected to be logarithmic against the horizon. To check if the relation is logarithmic, one can use the semilogx function in matplotlib.pyplot\n",
        "2. When the regret is not logarithmic, please check against the analysis, and obtain insights there for debugging.\n",
        "3. Similar to ETC, to see a smooth curve, one would have to repeat the simulation for about 10k times. One can either use parallel implementation, or run a smaller number of simulation for debug first.\n"
      ]
    }
  ]
}