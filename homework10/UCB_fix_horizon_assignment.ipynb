{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UCB-fix-horizon-assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/utaustin_optimization/blob/main/homework10/UCB_fix_horizon_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdnGqr85avQV"
      },
      "source": [
        "##Upper Confidence Bound (fixed horizon)\n",
        "\n",
        "In this excercise, we will be playing with the Multi-arm bandit problem with the UCB algorithm.\n",
        "\n",
        "## Setup\n",
        "\n",
        "As before, consider unstructural bandit problem. Suppose we have $k$ arms, each with random rewards $p_i = u_i + \\epsilon$ where $\\epsilon$ is draw from i.i.d. standard gaussian. (Note that we only require $\\epsilon$ to be sub-gaussian for the analysis to go through)\n",
        "\n",
        "The following codes is capturing the setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7C3IVx_ha2C"
      },
      "source": [
        "import numpy as np\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Gaussian_Arm:\n",
        "  def __init__(self, num_arms, mu=None):\n",
        "    '''\n",
        "    num_arms: (int). the number of arms\n",
        "    mu: (None or list-type). the mean of the reward of each arm.\n",
        "        if set to None, a random vector will be generated.\n",
        "    '''\n",
        "    if num_arms <= 1 or not isinstance(num_arms, int):\n",
        "      print('number of arms has an int that is at least two')\n",
        "      return\n",
        "    \n",
        "    self.num_arms = num_arms\n",
        "    #\n",
        "    if mu:\n",
        "      self.mu = np.asarray(mu)\n",
        "      if len(self.mu) != num_arms:\n",
        "        print('The lenth of mu does not match the number of arms')\n",
        "        return\n",
        "    else:\n",
        "      self.mu = np.random.rand((num_arms))\n",
        "    # \n",
        "    self.delta = max(self.mu) - min(self.mu)\n",
        "    #\n",
        "\n",
        "    # keep track of the rewards for the user\n",
        "    self.rewards_history = []\n",
        "    # keep track of how many times the arms have been pulled\n",
        "    self.total_pull = 0 \n",
        "\n",
        "  def pull_arm(self, arm_id=-1, pull_time=1):\n",
        "    if arm_id < 0 or arm_id >= self.num_arms:\n",
        "      print('please specify arm id in the range of 0-%d' % (self.num_arms))\n",
        "      return\n",
        "    assert (isinstance(pull_time, int) and pull_time >= 1)\n",
        "    self.total_pull += pull_time\n",
        "    # Generate reward\n",
        "    reward = self.mu[arm_id] * pull_time + sum(np.random.randn(pull_time))\n",
        "    self.rewards_history.append(reward)\n",
        "    return reward\n",
        "\n",
        "\n",
        "  def genie_reward(self):\n",
        "    '''\n",
        "    the best expected reward after pulling self.total_pull times\n",
        "    '''\n",
        "    best_mu = max(self.mu)\n",
        "    return self.total_pull * best_mu\n",
        "\n",
        "  def my_rewards(self):\n",
        "    return sum(self.rewards_history)\n",
        "\n",
        "  def clear_reward_hist(self):\n",
        "    self.rewards_history = []\n",
        "    self.total_pull = 0\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRIP2GfWhd1S"
      },
      "source": [
        "\n",
        "## Goal of this exercise\n",
        "1. Basic UCB algorithm implementation\n",
        "2. Plot the regret VS horizon ($n$).\n",
        "\n",
        "Please refer to previous assignment for tips.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wrQnYpZqi2G"
      },
      "source": [
        "# 1. Basic UCB algorithm implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHOoCT4Ct3tp"
      },
      "source": [
        "\n",
        "\n",
        "NUM_ARMS = 2\n",
        "\n",
        "class Empirical_mean:\n",
        "    def __init__(self,num_arms):\n",
        "        self.rewards = np.zeros(num_arms)\n",
        "        self.count = np.zeros(num_arms)\n",
        "        self.mean =  np.zeros(num_arms)\n",
        "    def add_new_reward(self, reward, indx):\n",
        "        self.rewards[indx] += reward\n",
        "        self.count[indx] += 1\n",
        "        self.mean[indx] = self.rewards[indx] / self.count[indx]\n",
        "    def mean(self, indx):\n",
        "        return self.mean[indx]\n",
        "    def reset(self):\n",
        "        self.rewards = self.rewards * 0 \n",
        "        self.count = self.rewards * 0 \n",
        "        self.mean =  self.rewards * 0 \n",
        "\n",
        "\n",
        "\n",
        "def UCB(arm, N, num_arms=NUM_ARMS):\n",
        "  bonus = lambda delta, t: np.sqrt((2*np.log(1/delta))/t) if t > 0 else float('inf')\n",
        "  delta = 1/N**2 \n",
        "  mean_vals = Empirical_mean(num_arms)\n",
        "  for i in range(N):\n",
        "      UCBbest_arm = np.argmax([u + bonus(delta, t) for (u,t) in zip(mean_vals.mean, mean_vals.count)])\n",
        "      mean_vals.add_new_reward(arm.pull_arm(UCBbest_arm), UCBbest_arm)\n",
        "  return arm.my_rewards()\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgjknPzct9wy"
      },
      "source": [
        "\n",
        "def regret_vs_horizon(Ns:list, REPEAT:int, algorithm: type(lambda x: None)):\n",
        "  regret = []\n",
        "  mu = [0.1, 0.0]\n",
        "  my_arm = Gaussian_Arm(NUM_ARMS, mu=mu)\n",
        "  for NUM_RUNs in Ns:\n",
        "    print(NUM_RUNs)\n",
        "    cur_regret = 0\n",
        "    for repeat in range(REPEAT):\n",
        "        rewards = algorithm(my_arm, NUM_RUNs, my_arm.num_arms) ## everyrun NUM_RUNs += 10000\n",
        "        cur_regret += my_arm.genie_reward() - rewards\n",
        "        my_arm.clear_reward_hist()\n",
        "    cur_regret /= REPEAT\n",
        "    regret.append(cur_regret)\n",
        "    #\n",
        "  return regret\n",
        "\n",
        "\n",
        "import plotly.graph_objects as graph\n",
        "def plot(all_history:list, title:str, log = False):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        all_history: list of dicts to plot\n",
        "    ret:\n",
        "        None: show plotly fig\n",
        "    \"\"\"\n",
        "    fig = graph.Figure(layout = graph.Layout(title=graph.layout.Title(text=title))) \n",
        "    for i in range(len(all_history)):\n",
        "        fig.add_trace(graph.Scatter(x = all_history[i][\"x\"], \n",
        "                                    y = all_history[i][\"y\"],\n",
        "                                    name = all_history[i][\"legend\"])) \n",
        "    if log: fig.update_xaxes(type=\"log\")\n",
        "    fig.show()\n",
        "\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqxDddGGqqJF"
      },
      "source": [
        "# 2. Plot the regret VS horizon ( ùëõ )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y42NkyFv40W1",
        "outputId": "1bc16321-0e9f-47e0-d8f5-dca1b856cccf"
      },
      "source": [
        "\n",
        "Ninit = 200\n",
        "Ns  = [Ninit * (2**i) for i in range(1, 7)]\n",
        "UCB_regret = regret_vs_horizon(Ns, REPEAT=200, algorithm=UCB)\n",
        "\n",
        "\n",
        "plot_UCB_regret = {\"legend\": \"mean_UBC_regret\", \n",
        "                   \"x\": Ns , \n",
        "                   \"y\": UCB_regret}\n",
        "\n",
        "plot([plot_UCB_regret], title=\"regret VS horizon - linear\" , log = False)\n",
        "plot([plot_UCB_regret], title=\"regret VS horizon - Log\" , log = True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400\n",
            "800\n",
            "1600\n",
            "3200\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}