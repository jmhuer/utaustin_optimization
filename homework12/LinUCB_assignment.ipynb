{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinUCB-assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/utaustin_optimization/blob/main/homework12/LinUCB_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaImnLHsjLbX"
      },
      "source": [
        "# LinUCB\n",
        "\n",
        "In this exercise, we will start looking at linear bandit with finite arms.\n",
        "\n",
        "The set-up is captured by the following python class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xujBvOrIjKzo"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pylab as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# (4 choose 2)\n",
        "available_arms = np.array([\n",
        "  (1, 1, 0, 0),\n",
        "  (1, 0, 1, 0),\n",
        "  (1, 0, 0, 1),\n",
        "  (0, 1, 1, 0),\n",
        "  (0, 1, 0, 1),\n",
        "  (0, 0, 1, 1)])\n",
        "\n",
        "class Context_arm(object):\n",
        "  def __init__(self, available_arms=available_arms,gaussian_bandit=True):\n",
        "    self.available_arms = np.array(available_arms)\n",
        "    self.gaussian_bandit = gaussian_bandit\n",
        "    self.num_arms = len(self.available_arms)\n",
        "    self.theta = np.array((0.1, 0.2, 0.2, 0.3)) #this is what we want to learn\n",
        "    self.num_features = len(self.available_arms[0])\n",
        "\n",
        "    # keep track of the rewards for the user\n",
        "    self.rewards_history = []\n",
        "    # keep track of how many times the arms have been pulled\n",
        "    self.total_pull = 0 \n",
        "    \n",
        "  def pull_arm(self, arm_idx):\n",
        "    arm = self.available_arms[arm_idx]\n",
        "    if self.gaussian_bandit:\n",
        "      reward = self.theta.dot(arm) + np.random.randn() / 2 \n",
        "    else:  # Bernoulli bandit\n",
        "      reward = 1 if np.random.random()< self.theta.dot(arm) else 0\n",
        "    \n",
        "    self.total_pull+=1\n",
        "    self.rewards_history.append(reward)\n",
        "    return reward\n",
        "  \n",
        "  def genie_reward(self, arms=None):\n",
        "    if not arms:\n",
        "      arms = self.available_arms\n",
        "    if self.gaussian_bandit:\n",
        "      reward = np.max([self.theta.dot(arm) for arm in arms]) \n",
        "    else:  # Bernoulli bandit\n",
        "      reward = np.max([self.theta.dot(arm) for arm in arms])\n",
        "    return reward * self.total_pull\n",
        "\n",
        "  def my_rewards(self):\n",
        "    return sum(self.rewards_history)\n",
        "\n",
        "  def clear_reward_hist(self):\n",
        "    self.rewards_history = []\n",
        "    self.total_pull = 0"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3nSXhk9mTkt"
      },
      "source": [
        "#utils \n",
        "\n",
        "class Empirical_mean:\n",
        "    def __init__(self,num_arms):\n",
        "        self.rewards = np.zeros(num_arms)\n",
        "        self.count = np.zeros(num_arms)\n",
        "        self.mean =  np.zeros(num_arms)\n",
        "    def add_new_reward(self, reward, indx):\n",
        "        self.rewards[indx] += reward\n",
        "        self.count[indx] += 1\n",
        "        self.mean[indx] = self.rewards[indx] / self.count[indx]\n",
        "    def mean(self, indx):\n",
        "        return self.mean[indx]\n",
        "    def reset(self):\n",
        "        self.rewards = self.rewards * 0 \n",
        "        self.count = self.rewards * 0 \n",
        "        self.mean =  self.rewards * 0 "
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUKUOx41jMnO"
      },
      "source": [
        "\n",
        "## Goal of this exercise\n",
        "1. Basic LinUCB algorithm implementation, for both Gaussian Reward and Bernoulli rewards\n",
        "2. Plot the regret VS horizon ($n$).\n",
        "3. Compare LinUCB with original UCB (for Gaussian rewards only) and KL-UCB (for Bernoulli rewards only)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSPwYO6gUZpF"
      },
      "source": [
        "# 1. Basic LinUCB algorithm implementation, for both Gaussian Reward and Bernoulli rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84FbkZwRViha"
      },
      "source": [
        "## LinUBC implemented below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd6P3aLgwO5R"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def LinUCB(arm, N): \n",
        "  ##pre alg calculations\n",
        "  delta = 1/N**2 #desiree confidence\n",
        "  alpha = 1 + np.sqrt(np.log(2/delta) / 2)\n",
        "  bonus = lambda x, A, a, t: 2 * np.sqrt(x.T @ (np.linalg.inv(A[a]) @ x)) \n",
        "\n",
        "  #I will initialize A, b before main loop for simplicity, yahoo paper does it when new actions show up\n",
        "  A = [np.identity(arm.num_features) for i in range(arm.num_arms)]\n",
        "  b = [np.zeros(arm.num_features) for i in range(arm.num_arms)]\n",
        "\n",
        "  # MAIN LOOOP\n",
        "  for t in range(N):\n",
        "      thetas = [np.linalg.inv(Aa) @ ba for (Aa, ba) in zip(A, b)] #disjoint implementation meaning thetas are not shared accross arms \n",
        "      confidence_intervals = np.array([(theta.T @ xt) + bonus(xt, A, a, t) for (xt, a, theta) in zip(arm.available_arms, range(arm.num_arms), thetas)])\n",
        "      UCBbest_arms = np.argmax(confidence_intervals)\n",
        "      rt = arm.pull_arm(UCBbest_arms)\n",
        "      A[UCBbest_arms] += np.outer(arm.available_arms[UCBbest_arms], arm.available_arms[UCBbest_arms].T) \n",
        "      b[UCBbest_arms] += rt * arm.available_arms[UCBbest_arms]\n",
        "\n",
        "  # DONE\n",
        "  return arm.my_rewards()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FW_K2B5VmU_"
      },
      "source": [
        "## UCB implemented below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbNR2uJfx-60"
      },
      "source": [
        "\n",
        "def UCB(arm, N):\n",
        "  bonus = lambda delta, t: np.sqrt((2*np.log(1/delta))/t) if t > 0 else float('inf')\n",
        "  delta = 1/N**2 \n",
        "  mean_vals = Empirical_mean(arm.num_arms)\n",
        "  for i in range(N):\n",
        "      UCBbest_arm = np.argmax([u + bonus(delta, t) for (u,t) in zip(mean_vals.mean, mean_vals.count)])\n",
        "      mean_vals.add_new_reward(arm.pull_arm(UCBbest_arm), UCBbest_arm)\n",
        "  return arm.my_rewards()\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjHj6tc_Vo5O"
      },
      "source": [
        "## KL-UBC implemented below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UQK6KY9mnWa"
      },
      "source": [
        "  #(relative entropy or KL divergence defined below)\n",
        "  def klBern(x, y):\n",
        "    eps = 1e-15 \n",
        "    x = min(max(x, eps), 1 - eps)\n",
        "    y = min(max(y, eps), 1 - eps)\n",
        "    return x*np.log(x/y) + (1-x)*np.log((1-x) / (1-y))\n",
        "\n",
        "def dkl_bernoulli(p, q):\n",
        "    eps = 1e-15 \n",
        "    p = min(max(p, eps), 1 - eps)\n",
        "    q = min(max(q, eps), 1 - eps)\n",
        "    result = (q-p)/(q*(1.0-q))\n",
        "    return result\n",
        "\n",
        "def max_newton(kl_distance, empiral_mean, k, t,dkl, precision = 1e-6, max_iterations = 50):\n",
        "    Nk = empiral_mean.count[k]\n",
        "    Sk = empiral_mean.rewards[k]\n",
        "    delta = 0.1\n",
        "    logtdt = np.log(t)/Nk\n",
        "    p = max(Sk/Nk, delta)\n",
        "    if p>=1: return 1\n",
        "    q = p + delta\n",
        "    for n in range(max_iterations):\n",
        "        f  = logtdt - kl_distance(p, q)\n",
        "        df = - dkl(p, q)\n",
        "        if f*f < precision: break\n",
        "    q = min(1 - delta , max(q - f / df, p + delta))\n",
        "    return q\n",
        "\n",
        "def KLUCB(arm, N):\n",
        "    max_u = lambda k,t: max_newton(klBern,mean_vals, k, t, dkl_bernoulli) if t > 0 else float('inf')\n",
        "    mean_vals = Empirical_mean(arm.num_arms)\n",
        "    for i in range(N):\n",
        "        UCBbest_arm = np.argmax([max_u(k,i) for k in range(arm.num_arms)])\n",
        "        mean_vals.add_new_reward(arm.pull_arm(UCBbest_arm), UCBbest_arm)\n",
        "    return arm.my_rewards()"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9G8k4ezVs3_"
      },
      "source": [
        "## Utils for plotting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9QJYhNPwQnV"
      },
      "source": [
        "\n",
        "def regret_vs_horizon(arm, Ns:list, REPEAT:int, algorithm: type(lambda x: None)):\n",
        "  regret = []\n",
        "  my_arm = arm\n",
        "  for NUM_RUNs in Ns:\n",
        "    print(NUM_RUNs)\n",
        "    cur_regret = 0\n",
        "    for repeat in range(REPEAT):\n",
        "        rewards = algorithm(my_arm, NUM_RUNs) ## everyrun NUM_RUNs += 10000\n",
        "        cur_regret += my_arm.genie_reward() - rewards\n",
        "        my_arm.clear_reward_hist()\n",
        "    cur_regret /= REPEAT\n",
        "    regret.append(cur_regret)\n",
        "  return regret\n",
        "\n",
        "\n",
        "import plotly.graph_objects as graph\n",
        "def plot(all_history:list, title:str, log = False):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        all_history: list of dicts to plot\n",
        "    ret:\n",
        "        None: show plotly fig\n",
        "    \"\"\"\n",
        "    fig = graph.Figure(layout = graph.Layout(title=graph.layout.Title(text=title))) \n",
        "    for i in range(len(all_history)):\n",
        "        fig.add_trace(graph.Scatter(x = all_history[i][\"x\"], \n",
        "                                    y = all_history[i][\"y\"],\n",
        "                                    name = all_history[i][\"legend\"])) \n",
        "    if log: fig.update_xaxes(type=\"log\")\n",
        "    fig.show()\n",
        "\n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zALOWTjVUiVE"
      },
      "source": [
        "# 2. Plot the regret VS horizon ($n$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhhU4uQdUkSx"
      },
      "source": [
        "\n",
        "Ninit = 100\n",
        "Ns  = [Ninit * (2**i) for i in range(1, 7)]\n",
        "\n",
        "gauss_arm     = Context_arm(gaussian_bandit=True)\n",
        "bernoulli_arm = Context_arm(gaussian_bandit=False)\n",
        "\n",
        "\n",
        "bern_LinUCB_regret = regret_vs_horizon(bernoulli_arm, Ns, REPEAT=100, algorithm=LinUCB)\n",
        "LinUCB_regret      = regret_vs_horizon(gauss_arm, Ns, REPEAT=100, algorithm=LinUCB)\n",
        "\n",
        "\n",
        "plot_LinUCB_regret = {\"legend\": \"mean_LinUBC_regret\", \n",
        "                      \"x\": Ns , \n",
        "                      \"y\": LinUCB_regret}\n",
        "\n",
        "plot_bern_LinUCB_regret = {\"legend\": \"mean_bern_LinUCB_regret\", \n",
        "                           \"x\": Ns , \n",
        "                           \"y\": bern_LinUCB_regret}\n",
        "\n",
        "\n",
        "plot([plot_LinUCB_regret], title=\"regret VS horizon - linear\", log = False)\n",
        "plot([plot_bern_LinUCB_regret], title=\"regret VS horizon - Log\", log = False)\n",
        "# plot([plot_LinUCB_regret], title=\"regret VS horizon - linear\", log = True)\n",
        "# plot([plot_bern_LinUCB_regret], title=\"regret VS horizon - Log\", log = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC34LDOXIVGj"
      },
      "source": [
        "# 3a. Compare LinUCB with original UCB (for Gaussian rewards only) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNHS0IxrwSpE"
      },
      "source": [
        "\n",
        "Ninit = 100\n",
        "Ns  = [Ninit * (2**i) for i in range(1, 7)]\n",
        "\n",
        "gauss_arm = Context_arm(gaussian_bandit=True)\n",
        "\n",
        "UCB_regret = regret_vs_horizon(gauss_arm, Ns, REPEAT=100, algorithm=UCB)\n",
        "LinUCB_regret = regret_vs_horizon(gauss_arm, Ns, REPEAT=100, algorithm=LinUCB)\n",
        "\n",
        "plot_LinUCB_regret = {\"legend\": \"mean_LinUBC_regret\", \n",
        "                      \"x\": Ns , \n",
        "                      \"y\": LinUCB_regret}\n",
        "\n",
        "plot_UCB_regret = {\"legend\": \"mean_UBC_regret\", \n",
        "                    \"x\": Ns , \n",
        "                    \"y\": UCB_regret}\n",
        "\n",
        "\n",
        "\n",
        "plot([plot_UCB_regret,plot_LinUCB_regret], title=\"regret VS horizon - linear\", log = False)\n",
        "# plot([plot_UCB_regret,plot_LinUCB_regret], title=\"regret VS horizon - Log\", log = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccsz5PTCIcag"
      },
      "source": [
        "# 3b. Compare LinUCB with KL-UCB (for Bernoulli rewards only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrePWEcqIjhY"
      },
      "source": [
        "\n",
        "Ninit = 100\n",
        "Ns  = [Ninit * (2**i) for i in range(1, 7)]\n",
        "\n",
        "bernoulli_arm = Context_arm(gaussian_bandit=False)\n",
        "\n",
        "KLUCB_regret = regret_vs_horizon(bernoulli_arm, Ns, REPEAT=100, algorithm=KLUCB)\n",
        "LinUCB_regret = regret_vs_horizon(bernoulli_arm, Ns, REPEAT=100, algorithm=LinUCB)\n",
        "\n",
        "plot_LinUCB_regret = {\"legend\": \"mean_LinUBC_regret\", \n",
        "                      \"x\": Ns , \n",
        "                      \"y\": LinUCB_regret}\n",
        "\n",
        "plot_KLUCB_regret = {\"legend\": \"mean_KLUCB_regret\", \n",
        "                    \"x\": Ns , \n",
        "                    \"y\": KLUCB_regret}\n",
        "\n",
        "plot([plot_LinUCB_regret,plot_KLUCB_regret], title=\"regret VS horizon - linear\" , log = False)\n",
        "# plot([plot_LinUCB_regret,plot_KLUCB_regret], title=\"regret VS horizon - Log\" , log = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}