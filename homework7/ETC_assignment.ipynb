{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ETC-assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/utaustin_optimization/blob/main/homework7/ETC_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DVUkrr5ap39"
      },
      "source": [
        "# Explore then Commit (ETC)\n",
        "\n",
        "In this excercises, we will be playing with the Multi-arm bandit problem with Explore then Commit algorithm.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Consider unstructural bandit problem. Suppose we have $k$ arms, each with random rewards $p_i = u_i + \\epsilon$ where $\\epsilon$ is draw from i.i.d. standard gaussian. (Note that we only require $\\epsilon$ to be sub-gaussian for the analysis to go through)\n",
        "\n",
        "The following codes is capturing the setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mve-Cs7ODbfs"
      },
      "source": [
        "import numpy as np\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Gaussian_Arm:\n",
        "  def __init__(self, num_arms, mu=None):\n",
        "    '''\n",
        "    num_arms: (int). the number of arms\n",
        "    mu: (None or list-type). the mean of the reward of each arm.\n",
        "        if set to None, a random vector will be generated.\n",
        "    '''\n",
        "    if num_arms <= 1 or not isinstance(num_arms, int):\n",
        "      print('number of arms has an int that is at least two')\n",
        "      return\n",
        "    \n",
        "    self.num_arms = num_arms\n",
        "    #\n",
        "    if mu:\n",
        "      self.mu = np.asarray(mu)\n",
        "      if len(self.mu) != num_arms:\n",
        "        print('The lenth of mu does not match the number of arms')\n",
        "        return\n",
        "    else:\n",
        "      self.mu = np.random.rand((num_arms))\n",
        "    # \n",
        "    self.delta = max(self.mu) - min(self.mu)\n",
        "    #\n",
        "\n",
        "    # keep track of the rewards for the user\n",
        "    self.rewards_history = []\n",
        "    # keep track of how many times the arms have been pulled\n",
        "    self.total_pull = 0 \n",
        "\n",
        "  def pull_arm(self, arm_id=-1, pull_time=1):\n",
        "    if arm_id < 0 or arm_id >= self.num_arms:\n",
        "      print('please specify arm id in the range of 0-%d' % (self.num_arms))\n",
        "      return\n",
        "    assert (isinstance(pull_time, int) and pull_time >= 1)\n",
        "    self.total_pull += pull_time\n",
        "    # Generate reward\n",
        "    reward = self.mu[arm_id] * pull_time + sum(np.random.randn(pull_time))\n",
        "    self.rewards_history.append(reward)\n",
        "    return reward\n",
        "\n",
        "\n",
        "  def genie_reward(self):\n",
        "    '''\n",
        "    the best expected reward after pulling self.total_pull times\n",
        "    '''\n",
        "    best_mu = max(self.mu)\n",
        "    return self.total_pull * best_mu\n",
        "\n",
        "  def my_rewards(self):\n",
        "    return sum(self.rewards_history)\n",
        "\n",
        "  def clear_reward_hist(self):\n",
        "    self.rewards_history = []\n",
        "    self.total_pull = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce_E9sfwDlgo"
      },
      "source": [
        "## Algorithm review\n",
        "\n",
        "(Please refer to the lecture notes and the text book for details)\n",
        "\n",
        "The parameter to set: the exploration time m\\*k\n",
        "\n",
        "1. Play each arm in the round-robin fashion until each arm are played m times\n",
        "2. Compute the empirical reward estimation for each arm\n",
        "3. Play the best arm (according to the empirical reward estimation) until the end of the game\n",
        "\n",
        "## Goal of these exercises\n",
        "\n",
        "Implement the following:\n",
        "\n",
        "1. Basic ETC algorithm implementation\n",
        "2. Plot the expected regret of ETC VS horizon ($n$).\n",
        "3. The doubling trick.\n",
        "4. Plot the expected regret of ETC with doubling trick VS horizon ($n$).\n",
        "\n",
        "Answer the following:\n",
        "\n",
        "1. What are the pros and cons of the doubling trick?\n",
        "2. Does the regret VS horizon plot looks like $\\log n$?\n",
        "3. What if the exploration time is not set appropriately  (which means, we explore too little or too much) (Usually because the sub-optimality gap cannot be estimated appropriately)?\n",
        "4. (Open-ended) Can we improve ETC?\n",
        "\n",
        "## Tips:\n",
        "1. The regret is expected to be logarithmic against the horizon. To check if the relationship is logarithmic, one can use the semilogx function in matplotlib.pyplot\n",
        "2. When the regret is not logarithmic, please check against the analysis, and obtain insights there for debugging.\n",
        "3. To see a smooth curve, one would have to repeat the simulation for multiple time, and take the empirical mean of the regret. This may be slow, if implemented on a single process. So please try-out parallel implementation, if you are comfortable with it (the simulation is very parallelable).\n"
      ]
    }
  ]
}