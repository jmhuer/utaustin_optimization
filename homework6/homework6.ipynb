{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/utaustin_optimization/blob/main/homework6/homework6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpTRNuz3KDWV"
      },
      "source": [
        "# Problem Set 6\n",
        "In this problem set you will implement SGD and SVRG and compare the two to each other, and also to GD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGbibBewhGy5",
        "outputId": "63ae10af-a73b-4a66-d58f-08bd10b1998e"
      },
      "source": [
        "!pip install gdown\n",
        "! git clone https://github.com/jmhuer/optimization_tools\n",
        "from optimization_tools.utils import download_gdrive\n",
        "\n",
        "# we pass the ID between two slashes '/{ID}/'  \n",
        "digits = '1X4fvOpQ2LK81-D3Fw4Oxugdvgm1dDJ3d'\n",
        "news = '1Sc6ew-Ti8uNAPScD9P1UXPXrziPqaEwd'\n",
        "download_gdrive(digits)\n",
        "download_gdrive(news)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "fatal: destination path 'optimization_tools' already exists and is not an empty directory.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1X4fvOpQ2LK81-D3Fw4Oxugdvgm1dDJ3d\n",
            "To: /content/digits.zip\n",
            "100%|##########| 98.7k/98.7k [00:00<00:00, 36.3MB/s]\n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Sc6ew-Ti8uNAPScD9P1UXPXrziPqaEwd\n",
            "To: /content/news.zip\n",
            "8.53MB [00:00, 134MB/s]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qt5K_-r6-rf"
      },
      "source": [
        "# Problem 1: Stochastic Variance Reduced Gradient Descent (SVRG)\n",
        "\n",
        "As we discussed in the video lectures, decomposable functions of the form\n",
        "$$\n",
        "\\min_{\\omega} \\left [ F(\\omega) = \\frac{1}{n} \\sum_i^n f_i(\\omega) \\right ],\n",
        "$$\n",
        "are very common in statistics/ML problems. Here, each $f_i$ corresponds to a loss for a particular training example. For\n",
        "example, if $f_i(\\omega) = (\\omega^\\top x_i - y_i)^2$, then $F(\\omega)$ is a least\n",
        "squares regression problem. The standard gradient descent (GD) update \n",
        "$$\n",
        "\\omega_t = \\omega_{t-1} - \\eta_t \\nabla F(\\omega_{t-1})\n",
        "$$\n",
        "\n",
        "evaluates the full gradient $\\nabla F(\\omega) = \\frac{1}{n} \\sum_i^n \\nabla\n",
        "f_i(\\omega)$, which requires evaluating $n$ derivatives. This can be\n",
        "prohibitively expensive when the number of training examples $n$ is large. SGD evaluates\n",
        "the gradient of one (or a small subset) of the training examples--drawn\n",
        "randomly from ${1,...n}$--per iteration:\n",
        "$$\n",
        "\\omega_t = \\omega_{t-1} - \\eta_t \\nabla f_i(\\omega_{t-1}).\n",
        "$$\n",
        "\n",
        "In expectation, the updates are equivalent, but SGD has the computational\n",
        "advantage of only evaluating a single gradient $\\nabla f_i(\\omega)$. The\n",
        "disadvatage is that the randomness introduces variance, which slows\n",
        "convergence. This was our motivation in class to introduce the SVRG algorithm.\n",
        "\n",
        "Given the dataset in **digits.zip**, plot the performance of GD, SGD, and SVRG for logistic regression with $l2$ regularization in terms of negative log likelihood on the training data against the number of gradient evaluations for a single training example (GD performs $n$ such evaluations per iteration and SGD performs $1$). Choose the $l2$ parameter to optimize performance on the test set. How does the choice of $T$ (the number of inner loops) affect the performance of SVRG? There should be one plot with a title and three lines with different colors, markers, and legend labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI9ZbgO2w7AV",
        "outputId": "47dff255-6237-49eb-a4c8-e175a2f68de6"
      },
      "source": [
        "import zipfile as zipfile\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import numpy.linalg as la\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import time\r\n",
        "import pdb\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "#sample code to load digits.zip\r\n",
        "def loaddata(filename):\r\n",
        "    data={}\r\n",
        "    with zipfile.ZipFile(filename) as z:\r\n",
        "        for filename in z.namelist():\r\n",
        "          data[filename] = pd.read_csv(z.open(filename), sep=' ', header=None)\r\n",
        "    return data\r\n",
        "\r\n",
        "digits_dict = loaddata('./digits.zip')\r\n",
        "print(digits_dict.keys())\r\n",
        "X_digits_train = digits_dict['X_digits_train.csv']\r\n",
        "X_digits_test = digits_dict['X_digits_test.csv']\r\n",
        "y_digits_train = digits_dict['y_digits_train.csv'].to_numpy(dtype=int).ravel()\r\n",
        "y_digits_test = digits_dict['y_digits_test.csv'].to_numpy(dtype=int).ravel()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['X_digits_test.csv', 'X_digits_train.csv', 'y_digits_test.csv', 'y_digits_train.csv'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VioAnFlQlli_"
      },
      "source": [
        "import numpy as np\n",
        "import numpy.random as rn\n",
        "import numpy.linalg as la\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import plotly.graph_objects as graph\n",
        "import numpy.linalg as la\n",
        "from tqdm import tqdm\n",
        "\n",
        "class MSE:\n",
        "    def __init__(self, train_data, test_data):\n",
        "        A ,b = train_data['data'], train_data['labels']\n",
        "        self.loss = lambda weight : (1./b.shape[0])*(.5)*np.sum((np.dot(A,weight)-b)**2)\n",
        "        self.grad_loss = lambda weight : (1./b.shape[0])*np.dot(A.T,np.dot(A,weight)-b)\n",
        "        #test loss\n",
        "        A_test ,b_test = test_data['data'], test_data['labels']\n",
        "        self.test_loss = lambda weights : (1./b_test.shape[0])*(.5)*np.sum((np.dot(A_test,weights)-b_test)**2)\n",
        "    def eval(self, weight):\n",
        "        return self.loss(weight) \n",
        "    def gradient(self,weight):\n",
        "        return self.grad_loss(weight) \n",
        "    def test_eval(self, weights):\n",
        "        return self.test_loss(weights) \n",
        "\n",
        "\n",
        "class SVRG:\n",
        "    def __init__(self, A, b, lmda = 1e-3):\n",
        "        self.lmda = lmda\n",
        "        self.loss = lambda weight : (1./b.shape[0])*(.5)*np.sum((np.dot(A,weight)-b)**2)\n",
        "        self.reg  = lambda weight : self.lmda*la.norm(weight,1)\n",
        "        self.grad_loss = lambda weight : (1./b.shape[0])*np.dot(A.T,np.dot(A,weight)-b)\n",
        "        self.grad_reg  = lambda weight : self.lmda*la.norm(weight,1) \n",
        "    def eval(self, weight):\n",
        "        return self.loss(weight) \n",
        "    def gradient(self,weight):\n",
        "        return self.grad_loss(weight) \n",
        "\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSMBVd3Mk9eX"
      },
      "source": [
        "class GD:\n",
        "    def __init__(self, function, x_init, epochs, learning_rate):\n",
        "        self.function = function\n",
        "        self.x_init = x_init\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.history = {\"step\": [],\n",
        "                        \"function_vals\": [],\n",
        "                        \"grad_vals\": [], \n",
        "                        \"x_vals\": [self.x_init], \n",
        "                        \"opt_dif\": [],\n",
        "                        \"test_loss\": []}\n",
        "    def go(self, test = None):\n",
        "        for i in tqdm(range(self.epochs)):\n",
        "          ##RETRIVE OLD VALS\n",
        "          x_old = self.history['x_vals'][-1]\n",
        "          ##EVALUATE HERE\n",
        "          y = self.function.eval(x_old)\n",
        "          g = self.function.gradient(x_old)\n",
        "          ##UPDATE HERE \n",
        "          x = x_old - self.learning_rate(i) * g\n",
        "          ##STORE HISTORY\n",
        "          self.history['step'].append(i)\n",
        "          self.history['function_vals'].append(float(y))\n",
        "          self.history['grad_vals'].append(g)\n",
        "          self.history['x_vals'].append(x)\n",
        "          if test:  \n",
        "              self.history['test_loss'].append(float(self.function.test_eval(x)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28Poj_GXyhKi"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "KrOVeOSumpP2",
        "outputId": "cb9a4eb7-0c2b-4626-cc67-7dc366efd485"
      },
      "source": [
        "def L_M(A):\n",
        "  sm , sc = (1/A.shape[0])*la.norm(A,2)**2,(1/A.shape[0])*la.norm(A,-2)**2\n",
        "  return sm , sc\n",
        "\n",
        "train_data = {\"data\"  : X_digits_train,\n",
        "              \"labels\": y_digits_train}\n",
        "\n",
        "test_data  = {\"data\"  : X_digits_test,\n",
        "              \"labels\": y_digits_test}\n",
        "\n",
        "sm , sc = L_M(X_digits_train)\n",
        "gd    = GD(MSE(train_data, test_data), x_init = np.random.rand(X_digits_train.shape[1])*0, epochs = int(5e2), learning_rate = lambda dummy : 1/sm)\n",
        "\n",
        "gd.go(test=True)\n",
        "\n",
        "# plot the train loss\n",
        "all_history = { \"GD\"  : gd.history }\n",
        "\n",
        "fig = graph.Figure(layout = graph.Layout(title=graph.layout.Title(text=\"GD\")))\n",
        "for i in all_history:\n",
        "    fig.add_trace(graph.Scatter(x    = all_history[i][\"step\"],\n",
        "                                y    = all_history[i][\"function_vals\"],\n",
        "                                name = i))\n",
        "fig.show()\n",
        "\n",
        "\n",
        "\n",
        "# # plot the test loss\n",
        "# fig = graph.Figure(layout = graph.Layout(title=graph.layout.Title(text=\"GD\")))\n",
        "# for i in all_history:\n",
        "#     fig.add_trace(graph.Scatter(x    = all_history[i][\"step\"],\n",
        "#                                 y    = all_history[i][\"test_loss\"],\n",
        "#                                 name = i))\n",
        "# fig.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 1358.58it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"9c39e5e8-5f08-4ece-943d-d3d416658b40\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"9c39e5e8-5f08-4ece-943d-d3d416658b40\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '9c39e5e8-5f08-4ece-943d-d3d416658b40',\n",
              "                        [{\"name\": \"GD\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [14.128130217028382, 4.084773348346907, 4.002614928495485, 3.924787548379353, 3.85098018312264, 3.780910043225146, 3.714319630617758, 3.6509741233209323, 3.5906590508128677, 3.533178226660386, 3.4783519088842136, 3.4260151619846506, 3.3760163975988937, 3.3282160734465744, 3.2824855325881646, 3.2387059671097793, 3.1967674921906455, 3.1565683181355437, 3.118014009389474, 3.081016820818434, 3.0454951026584145, 3.0113727665221206, 2.978578805724969, 2.947046863962251, 2.9167148470499393, 2.887524573043095, 2.8594214565774565, 2.8323542237497965, 2.8062746542682797, 2.781137347971664, 2.7568995131414282, 2.733520774318731, 2.7109629975928518, 2.689190131553339, 2.6681680622978363, 2.647864481064529, 2.6282487632149767, 2.6092918574320816, 2.5909661841212017, 2.573245542111747, 2.5561050228535804, 2.539520931388655, 2.523470713454759, 2.5079328881461542, 2.492886985616229, 2.478313489360949, 2.4641937826695925, 2.4505100988717214, 2.437245475047136, 2.424383708899234, 2.4119093185221967, 2.3998075048191714, 2.388064116352518, 2.3766656164284545, 2.3655990522375223, 2.3548520258892656, 2.3444126671947796, 2.334269608064381, 2.3244119583998795, 2.314829283371872, 2.305511581982301, 2.296449266821328, 2.287633144935544, 2.279054399731607, 2.270704573845885, 2.2625755529164406, 2.254659550198926, 2.2469490919726867, 2.2394370036876268, 2.232116396806296, 2.2249806562990937, 2.2180234287537437, 2.211238611063033, 2.204620339657448, 2.1981629802517633, 2.191861118076773, 2.185709548569421, 2.179703268496328, 2.17383746748747, 2.168107519958231, 2.1625089773994968, 2.1570375610167365, 2.1516891547002253, 2.146459798309637, 2.1413456812572824, 2.1363431363751797, 2.1314486340520338, 2.126658776626988, 2.1219702930277893, 2.1173800336416604, 2.1128849654078703, 2.1084821671215552, 2.10416882493893, 2.099942228074549, 2.0957997646817663, 2.091738917908004, 2.087757262116882, 2.083852459269644, 2.0800222554587195, 2.076264477586613, 2.0725770301836235, 2.0689578923582683, 2.06540511487452, 2.061916817350298, 2.0584911855719086, 2.055126468919352, 2.0518209778977012, 2.0485730817699457, 2.0453812062869186, 2.042243831510132, 2.039159489723536, 2.036126763430387, 2.033144283431607, 2.0302107269821543, 2.0273248160221033, 2.024485315479264, 2.021691031640318, 2.0189408105875946, 2.0162335366987123, 2.0135681312064526, 2.0109435508163487, 2.0083587863795715, 2.005812861618804, 2.003304831904904, 2.0008337830822422, 1.9983988303406879, 1.9959991171323261, 1.9936338141310415, 1.9913021182332074, 1.9890032515977836, 1.9867364607242026, 1.9845010155664882, 1.9822962086821259, 1.9801213544142544, 1.977975788105824, 1.975858865344407, 1.973769961236417, 1.971708469709538, 1.9696738028422125, 1.967665390219096, 1.9656826783114196, 1.9637251298812541, 1.9617922234087124, 1.9598834525411535, 1.9579983255635178, 1.9561363648889254, 1.9542971065687351, 1.9524800998212735, 1.9506849065784948, 1.9489111010498448, 1.9471582693026463, 1.9454260088583428, 1.9437139283039724, 1.9420216469182618, 1.940348794311759, 1.9386950100804485, 1.9370599434723172, 1.9354432530663517, 1.9338446064634798, 1.9322636799889865, 1.930700158405947, 1.929153734639247, 1.9276241095097717, 1.9261109914783685, 1.9246140963991942, 1.9231331472820854, 1.9216678740635977, 1.920218013386372, 1.9187833083865098, 1.9173635084886447, 1.9159583692084032, 1.9145676519619854, 1.913191123882571, 1.9118285576432998, 1.9104797312865707, 1.909144428059414, 1.9078224362547012, 1.9065135490579788, 1.905217564399696, 1.903934284812636, 1.9026635172943342, 1.9014050731743168, 1.9001587679859508, 1.8989244213427505, 1.8977018568189643, 1.8964909018342757, 1.8952913875424735, 1.8941031487239322, 1.8929260236817667, 1.8917598541415168, 1.890604485154236, 1.889459765002856, 1.8883255451116971, 1.8872016799590234, 1.8860880269925124, 1.8849844465475432, 1.8838908017681955, 1.8828069585308607, 1.88173278537036, 1.8806681534084984, 1.879612936284941, 1.8785670100903444, 1.8775302533016534, 1.8765025467194878, 1.8754837734075376, 1.8744738186339027, 1.8734725698143018, 1.8724799164570824, 1.871495750109969, 1.8705199643084913, 1.8695524545260223, 1.8685931181253828, 1.8676418543119395, 1.866698564088166, 1.865763150209591, 1.8648355171421038, 1.8639155710205597, 1.8630032196086426, 1.8620983722599436, 1.8612009398802045, 1.8603108348906983, 1.8594279711926975, 1.858552264132998, 1.8576836304704556, 1.8568219883435153, 1.85596725723868, 1.8551193579599, 1.854278212598851, 1.8534437445060623, 1.8526158782628777, 1.8517945396542104, 1.8509796556420774, 1.8501711543398733, 1.8493689649873708, 1.8485730179264175, 1.8477832445773097, 1.8469995774158172, 1.8462219499508399, 1.8454502967026751, 1.8446845531818779, 1.843924655868691, 1.8431705421930302, 1.842422150515003, 1.8416794201059474, 1.8409422911299733, 1.8402107046259883, 1.8394846024901956, 1.8387639274590455, 1.8380486230926318, 1.837338633758512, 1.836633904615947, 1.8359343816005345, 1.835240011409241, 1.8345507414858038, 1.8338665200065005, 1.8331872958662745, 1.8325130186652039, 1.8318436386952999, 1.8311791069276355, 1.83051937499978, 1.8298643952035443, 1.8292141204730166, 1.8285685043728874, 1.8279275010870513, 1.8272910654074797, 1.8266591527233524, 1.8260317190104491, 1.8254087208207816, 1.8247901152724715, 1.824175860039856, 1.823565913343825, 1.8229602339423732, 1.82235878112137, 1.8217615146855346, 1.8211683949496156, 1.8205793827297614, 1.8199944393350902, 1.819413526559437, 1.8188366066732866, 1.818263642415879, 1.8176945969874871, 1.817129434041862, 1.8165681176788373, 1.8160106124370907, 1.8154568832870612, 1.8149068956240137, 1.8143606152612497, 1.8138180084234585, 1.8132790417402065, 1.8127436822395595, 1.8122118973418377, 1.8116836548534974, 1.811158922961134, 1.8106376702256115, 1.810119865576302, 1.8096054783054472, 1.8090944780626288, 1.808586834849347, 1.808082519013707, 1.807581501245208, 1.807083752569637, 1.806589244344055, 1.8060979482518862, 1.805609836298095, 1.8051248808044618, 1.804643054404941, 1.8041643300411123, 1.8036886809577133, 1.8032160806982573, 1.8027465031007346, 1.8022799222933863, 1.8018163126905646, 1.8013556489886628, 1.8008979061621198, 1.8004430594595022, 1.7999910843996483, 1.7995419567678916, 1.799095652612341, 1.7986521482402371, 1.7982114202143655, 1.7977734453495373, 1.7973382007091308, 1.7969056636016916, 1.7964758115775945, 1.7960486224257608, 1.7956240741704337, 1.7952021450680085, 1.7947828136039161, 1.7943660584895609, 1.7939518586593102, 1.7935401932675317, 1.7931310416856867, 1.7927243834994628, 1.7923201985059654, 1.7919184667109451, 1.7915191683260774, 1.7911222837662857, 1.7907277936471075, 1.7903356787821014, 1.7899459201803014, 1.7895584990437081, 1.78917339676482, 1.7887905949242089, 1.7884100752881273, 1.78803181980616, 1.787655810608909, 1.7872820300057184, 1.7869104604824297, 1.786541084699177, 1.786173885488216, 1.7858088458517847, 1.7854459489599968, 1.7850851781487733, 1.7847265169177964, 1.7843699489285036, 1.7840154580021086, 1.7836630281176493, 1.7833126434100712, 1.7829642881683379, 1.782617946833565, 1.7822736039971903, 1.781931244399164, 1.7815908529261708, 1.7812524146098754, 1.7809159146251952, 1.7805813382885978, 1.7802486710564243, 1.7799178985232362, 1.7795890064201874, 1.7792619806134198, 1.7789368071024776, 1.7786134720187552, 1.7782919616239548, 1.7779722623085756, 1.7776543605904178, 1.777338243113116, 1.7770238966446819, 1.776711308076081, 1.7764004644198164, 1.7760913528085445, 1.7757839604936998, 1.7754782748441462, 1.7751742833448427, 1.7748719735955305, 1.7745713333094366, 1.7742723503119953, 1.7739750125395886, 1.773679308038303, 1.7733852249627018, 1.7730927515746182, 1.7728018762419608, 1.7725125874375374, 1.772224873737892, 1.771938723822165, 1.771654126470957, 1.7713710705652177, 1.771089545085147, 1.7708095391091068, 1.7705310418125528, 1.7702540424669768, 1.7699785304388653, 1.7697044951886702, 1.7694319262697926, 1.7691608133275831, 1.7688911460983512, 1.7686229144083898, 1.7683561081730124, 1.7680907173956026, 1.7678267321666743, 1.7675641426629474, 1.7673029391464323, 1.7670431119635266, 1.7667846515441261, 1.7665275484007426, 1.7662717931276373, 1.7660173763999634, 1.7657642889729166, 1.765512521680904, 1.765262065436715, 1.7650129112307074, 1.7647650501300032, 1.7645184732776933, 1.7642731718920528, 1.7640291372657664, 1.7637863607651632, 1.7635448338294597, 1.7633045479700147, 1.7630654947695914, 1.7628276658816278, 1.7625910530295201, 1.7623556480059104, 1.7621214426719845, 1.7618884289567807, 1.7616565988565012, 1.7614259444338418, 1.761196457817315, 1.7609681312005983, 1.7607409568418755, 1.7605149270631955, 1.7602900342498347, 1.7600662708496677, 1.7598436293725455, 1.7596221023896814, 1.7594016825330445, 1.7591823624947585, 1.7589641350265108, 1.7587469929389665, 1.7585309291011875, 1.7583159364400638, 1.7581020079397454, 1.7578891366410851, 1.7576773156410854, 1.757466538092354, 1.7572567972025637, 1.7570480862339193, 1.756840398502629, 1.7566337273783872, 1.756428066283856, 1.7562234086941584, 1.7560197481363733, 1.755817078189041, 1.7556153924816675, 1.7554146846942422, 1.7552149485567545, 1.755016177848721, 1.754818366398713, 1.7546215080838945, 1.7544255968295621, 1.7542306266086907, 1.7540365914414846, 1.7538434853949338, 1.753651302582375, 1.7534600371630575, 1.7532696833417138, 1.7530802353681363, 1.752891687536754, 1.7527040341862228, 1.7525172696990103, 1.7523313885009926, 1.7521463850610512, 1.7519622538906765, 1.7517789895435754, 1.7515965866152818, 1.751415039742774, 1.7512343436040918, 1.7510544929179646, 1.750875482443436, 1.750697306979498, 1.750519961364726, 1.7503434404769198, 1.7501677392327464, 1.74999285258739, 1.7498187755342014, 1.7496455031043536, 1.749473030366502, 1.7493013524264456, 1.749130464426794, 1.7489603615466378, 1.7487910390012193, 1.7486224920416138]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"GD\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9c39e5e8-5f08-4ece-943d-d3d416658b40');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFOdFTo57S98"
      },
      "source": [
        "# Problem 2: Newsgroup Dataset Optimization\n",
        "\n",
        "Using any approach, optimize performance of logistic regression on the test set in **news.zip** and compare the performance of your approach to standard SGD. This dataset is the full-dimensional newsgroup dataset (as opposed to the compressed version you worked with previously). The $X$ matrices are stored in sparse matrix format and can be read using scipy.io.mmread. As the dataset is large and high-dimensional, you will have to decide on how best to allocate your computational resources. Try to utilize the sparsity of the data (i.e., don't just convert it to a dense matrix and spend all your time multiplying zeros). You may use any of the techniques covered in class or ideas from outside class (e.g., momentum, variance reduction, minibatches, adaptive learning rates, preprocessing). Describe your methodology and comment on what you found improved performance and why. Plot the performance (negative log likelihood) of your method against standard SGD in terms of the number of gradient evaluations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF0Wc09XKhMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597c8575-d8d4-44ae-accc-3fd290667df0"
      },
      "source": [
        "from scipy.io import mmread\r\n",
        "import sklearn.feature_selection\r\n",
        "\r\n",
        "\r\n",
        "#sample code to load news.zip\r\n",
        "def loadnewsdata(filename='./news.zip'):\r\n",
        "    data={}\r\n",
        "    with zipfile.ZipFile(filename) as z:\r\n",
        "        for filename in z.namelist():\r\n",
        "          if 'csv' in filename:\r\n",
        "            data[filename] = pd.read_csv(z.open(filename), sep=' ', header=None)\r\n",
        "          elif 'mtx' in filename:\r\n",
        "            data[filename] = mmread(z.open(filename))\r\n",
        "          else:\r\n",
        "            raise Exception('unexpected filetype') \r\n",
        "    return data\r\n",
        "\r\n",
        "news_dict = loadnewsdata('./news.zip')\r\n",
        "print(news_dict.keys())\r\n",
        "X_news_train = news_dict['X_news_train.mtx']\r\n",
        "X_news_test = news_dict['X_news_test.mtx']\r\n",
        "y_news_train = news_dict['y_news_train.csv'].to_numpy(dtype=int).ravel()\r\n",
        "y_news_test = news_dict['y_news_test.csv'].to_numpy(dtype=int).ravel()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['X_news_test.mtx', 'X_news_train.mtx', 'y_news_test.csv', 'y_news_train.csv'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qqhhds9xa3G"
      },
      "source": [
        "class Logistic_loss:\n",
        "    def __init__(self, train_data, test_data, mou=0.001):\n",
        "        self.X = train_data['data']\n",
        "        self.Y = train_data['labels']\n",
        "        self.X_test = test_data['data']\n",
        "        self.Y_test = test_data['labels']\n",
        "        self.N = np.shape(self.X)\n",
        "        self.mou = mou\n",
        "        #pre_compute some quanities\n",
        "        Z_tr,Z_te=[],[]\n",
        "        for j in range(len(np.unique(self.Y))):\n",
        "            Z_tr.append(np.sum(X_news_train.toarray()[np.where(y_news_train==j)[0],:],axis=0))\n",
        "            Z_te.append(np.sum(X_news_test.toarray()[np.where(y_news_test==j)[0],:],axis=0))\n",
        "        self.Z = np.asarray(Z_tr).T\n",
        "        self.Z_test = np.asarray(Z_te).T\n",
        "    def eval(self, weights):\n",
        "        X,Y,Z=self.X,self.Y,self.Z\n",
        "        loss=(1./Y.shape[0])*(np.trace(np.dot(weights.T,Z))+np.sum(np.log(np.sum(np.exp(-np.dot(X,weights)),axis=1))))\n",
        "        reg=self.mou*la.norm(weights,'fro')**2\n",
        "        return loss + reg\n",
        "    def gradient(self,weights):\n",
        "        return self.subgradient(weights)\n",
        "    def subgradient(self,weights):\n",
        "        n,d= self.N\n",
        "        X,Y,Z=self.X,self.Y,self.Z\n",
        "        w=(np.exp(-np.dot(X,weights)).T/np.sum(np.exp(-np.dot(X,weights)),axis=1)).T\n",
        "        g_loss=(1./Y.shape[0])*(Z+np.dot(-X.T,w))\n",
        "        g_reg=2*self.mou*weights\n",
        "        return g_loss + g_reg\n",
        "    def test_eval(self, weights):\n",
        "        X = self.X_test \n",
        "        Y = self.Y_test   \n",
        "        Z = self.Z_test\n",
        "        loss=(1./Y.shape[0])*(np.trace(np.dot(weights.T,Z))+np.sum(np.log(np.sum(np.exp(-np.dot(X,weights)),axis=1))))\n",
        "        reg=self.mou*la.norm(weights,'fro')**2\n",
        "        return loss # no reg term for testing"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yubRDihAOr6"
      },
      "source": [
        "# print(type(X_digits_test.describe()))\n",
        "\n",
        "# X_news_test.describe()\n",
        "\n",
        "# print(np.shape(y_news_test))\n",
        "\n",
        "# print(np.shape(X_digits_test))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOXpfav51CmZ",
        "outputId": "a9cf0136-9dbf-4040-cbf1-28eca2eb9599"
      },
      "source": [
        "import scipy\n",
        "\n",
        "def L_M(A):\n",
        "  sm , sc = (1/A.shape[0])*la.norm(A,2)**2,(1/A.shape[0])*la.norm(A,-2)**2\n",
        "  return sm , sc\n",
        "  \n",
        "print(\"x_test\", X_news_test.toarray().shape)\n",
        "print(\"y_test\", y_news_test.shape)\n",
        "\n",
        "\n",
        "##Read data\n",
        "\n",
        "train_data = {\"data\"  : X_news_train.toarray(),\n",
        "              \"labels\": y_news_train}\n",
        "\n",
        "test_data  = {\"data\"  : X_news_test.toarray(),\n",
        "              \"labels\": y_news_test}\n",
        "\n",
        "# print( train_data['data'])\n",
        "# sm , sc = L_M(X_news_train.toarray())\n",
        "gd    = GD(Logistic_loss(train_data, test_data), x_init= np.zeros((101631,20)), epochs = int(5e1), learning_rate = lambda dummy : 0.01)\n",
        "\n",
        "gd.go(test=True)\n",
        "\n",
        "# plot the train loss\n",
        "all_history = { \"GD\"  : gd.history }\n",
        "\n",
        "fig = graph.Figure(layout = graph.Layout(title=graph.layout.Title(text=\"GD\")))\n",
        "for i in all_history:\n",
        "    fig.add_trace(graph.Scatter(x    = all_history[i][\"step\"],\n",
        "                                y    = all_history[i][\"function_vals\"],\n",
        "                                name = i))\n",
        "fig.show()\n",
        "\n",
        "\n",
        "\n",
        "# plot the test loss\n",
        "fig = graph.Figure(layout = graph.Layout(title=graph.layout.Title(text=\"GD\")))\n",
        "for i in all_history:\n",
        "    fig.add_trace(graph.Scatter(x    = all_history[i][\"step\"],\n",
        "                                y    = all_history[i][\"test_loss\"],\n",
        "                                name = i))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_test (6282, 101631)\n",
            "y_test (6282,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 29/50 [03:00<02:09,  6.17s/it]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}