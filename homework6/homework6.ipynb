{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/utaustin_optimization/blob/main/homework6/homework6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpTRNuz3KDWV"
      },
      "source": [
        "# Problem Set 6\n",
        "In this problem set you will implement SGD and SVRG and compare the two to each other, and also to GD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qt5K_-r6-rf"
      },
      "source": [
        "# Problem 1: Stochastic Variance Reduced Gradient Descent (SVRG)\n",
        "\n",
        "As we discussed in the video lectures, decomposable functions of the form\n",
        "$$\n",
        "\\min_{\\omega} \\left [ F(\\omega) = \\frac{1}{n} \\sum_i^n f_i(\\omega) \\right ],\n",
        "$$\n",
        "are very common in statistics/ML problems. Here, each $f_i$ corresponds to a loss for a particular training example. For\n",
        "example, if $f_i(\\omega) = (\\omega^\\top x_i - y_i)^2$, then $F(\\omega)$ is a least\n",
        "squares regression problem. The standard gradient descent (GD) update \n",
        "$$\n",
        "\\omega_t = \\omega_{t-1} - \\eta_t \\nabla F(\\omega_{t-1})\n",
        "$$\n",
        "\n",
        "evaluates the full gradient $\\nabla F(\\omega) = \\frac{1}{n} \\sum_i^n \\nabla\n",
        "f_i(\\omega)$, which requires evaluating $n$ derivatives. This can be\n",
        "prohibitively expensive when the number of training examples $n$ is large. SGD evaluates\n",
        "the gradient of one (or a small subset) of the training examples--drawn\n",
        "randomly from ${1,...n}$--per iteration:\n",
        "$$\n",
        "\\omega_t = \\omega_{t-1} - \\eta_t \\nabla f_i(\\omega_{t-1}).\n",
        "$$\n",
        "\n",
        "In expectation, the updates are equivalent, but SGD has the computational\n",
        "advantage of only evaluating a single gradient $\\nabla f_i(\\omega)$. The\n",
        "disadvatage is that the randomness introduces variance, which slows\n",
        "convergence. This was our motivation in class to introduce the SVRG algorithm.\n",
        "\n",
        "Given the dataset in **digits.zip**, plot the performance of GD, SGD, and SVRG for logistic regression with $l2$ regularization in terms of negative log likelihood on the training data against the number of gradient evaluations for a single training example (GD performs $n$ such evaluations per iteration and SGD performs $1$). Choose the $l2$ parameter to optimize performance on the test set. How does the choice of $T$ (the number of inner loops) affect the performance of SVRG? There should be one plot with a title and three lines with different colors, markers, and legend labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI9ZbgO2w7AV",
        "outputId": "40bca0cc-9ba0-4898-bf52-b708aef7ad8e"
      },
      "source": [
        "import zipfile as zipfile\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import numpy.linalg as la\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import time\r\n",
        "import pdb\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "#sample code to load digits.zip\r\n",
        "def loaddata(filename):\r\n",
        "    data={}\r\n",
        "    with zipfile.ZipFile(filename) as z:\r\n",
        "        for filename in z.namelist():\r\n",
        "          data[filename] = pd.read_csv(z.open(filename), sep=' ', header=None)\r\n",
        "    return data\r\n",
        "\r\n",
        "digits_dict = loaddata('./digits.zip')\r\n",
        "print(digits_dict.keys())\r\n",
        "X_digits_train = digits_dict['X_digits_train.csv']\r\n",
        "X_digits_test = digits_dict['X_digits_test.csv']\r\n",
        "y_digits_train = digits_dict['y_digits_train.csv'].to_numpy(dtype=int).ravel()\r\n",
        "y_digits_test = digits_dict['y_digits_test.csv'].to_numpy(dtype=int).ravel()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['X_digits_test.csv', 'X_digits_train.csv', 'y_digits_test.csv', 'y_digits_train.csv'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFOdFTo57S98"
      },
      "source": [
        "# Problem 2: Newsgroup Dataset Optimization\n",
        "\n",
        "Using any approach, optimize performance of logistic regression on the test set in **news.zip** and compare the performance of your approach to standard SGD. This dataset is the full-dimensional newsgroup dataset (as opposed to the compressed version you worked with previously). The $X$ matrices are stored in sparse matrix format and can be read using scipy.io.mmread. As the dataset is large and high-dimensional, you will have to decide on how best to allocate your computational resources. Try to utilize the sparsity of the data (i.e., don't just convert it to a dense matrix and spend all your time multiplying zeros). You may use any of the techniques covered in class or ideas from outside class (e.g., momentum, variance reduction, minibatches, adaptive learning rates, preprocessing). Describe your methodology and comment on what you found improved performance and why. Plot the performance (negative log likelihood) of your method against standard SGD in terms of the number of gradient evaluations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF0Wc09XKhMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1179b1-03a3-4e21-c8c6-6b97fc4cbfb1"
      },
      "source": [
        "from scipy.io import mmread\r\n",
        "import sklearn.feature_selection\r\n",
        "\r\n",
        "\r\n",
        "#sample code to load news.zip\r\n",
        "def loadnewsdata(filename='./news.zip'):\r\n",
        "    data={}\r\n",
        "    with zipfile.ZipFile(filename) as z:\r\n",
        "        for filename in z.namelist():\r\n",
        "          if 'csv' in filename:\r\n",
        "            data[filename] = pd.read_csv(z.open(filename), sep=' ', header=None)\r\n",
        "          elif 'mtx' in filename:\r\n",
        "            data[filename] = mmread(z.open(filename))\r\n",
        "          else:\r\n",
        "            raise Exception('unexpected filetype') \r\n",
        "    return data\r\n",
        "\r\n",
        "news_dict = loadnewsdata('./news.zip')\r\n",
        "print(news_dict.keys())\r\n",
        "X_news_train = news_dict['X_news_train.mtx']\r\n",
        "X_news_test = news_dict['X_news_test.mtx']\r\n",
        "y_news_train = news_dict['y_news_train.csv'].to_numpy(dtype=int).ravel()\r\n",
        "y_news_test = news_dict['y_news_test.csv'].to_numpy(dtype=int).ravel()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['X_news_test.mtx', 'X_news_train.mtx', 'y_news_test.csv', 'y_news_train.csv'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qqhhds9xa3G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}