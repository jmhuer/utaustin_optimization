{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exp3_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/utaustin_optimization/blob/main/homework11/Exp3_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Be43B9Zay2l"
      },
      "source": [
        "# Exponential Weight Algorithm for Explore and Exploit (EXP3)\n",
        "\n",
        "In this exercise, we will be studying the exponential weight algorithm for explore and exploit (EXP3).\n",
        "\n",
        "While this algorithm is designed for adversarial bandit setting, we will test this algorithm in the Bernoulli bandit setting, for the ease of implementation.\n",
        "\n",
        "As an example of the Bernoulli bandit, the following codes create a rewards history matrix $rewards$, such that $rewards[t, a]$ returns the reward you will get if you query arm $a$ at time $t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYtpMYxFs59D"
      },
      "source": [
        "import numpy as np\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Adverserial_Arm:\n",
        "  def __init__(self, num_arms):\n",
        "    '''\n",
        "    num_arms: (int). the number of arms\n",
        "    mu: (None or list-type). the mean of the reward of each arm.\n",
        "        if set to None, a random vector will be generated.\n",
        "    '''\n",
        "    if num_arms <= 1 or not isinstance(num_arms, int):\n",
        "      print('number of arms has an int that is at least two')\n",
        "      return\n",
        "    \n",
        "    self.num_arms = num_arms\n",
        "    self.reward_sequence = None #here we store adverserial setting\n",
        "    # self.create_reward_seqeunce(N=10000) #we create reward sequence automatically \n",
        "    # \n",
        "    # keep track of the rewards for the user\n",
        "    self.rewards_history = []\n",
        "    # keep track of how many times the arms have been pulled\n",
        "    self.total_pull = 0 \n",
        "  def create_reward_seqeunce(self, N): #no statistical/prob assumptions \n",
        "    numAction = self.num_arms \n",
        "    numRound = N\n",
        "    expected_rewards = np.arange(numAction) + 1\n",
        "    expected_rewards = 1 / expected_rewards\n",
        "    expected_rewards = np.repeat(expected_rewards.reshape(1,-1), numRound, axis=0)\n",
        "    rewards = np.random.rand(numRound, numAction) < expected_rewards\n",
        "    rewards = rewards.astype(float)\n",
        "    self.reward_sequence =  rewards\n",
        "  def pull_arm(self, time_step, arm_id=-1, pull_time=1):\n",
        "    if arm_id < 0 or arm_id >= self.num_arms:\n",
        "      print('please specify arm id in the range of 0-%d' % (self.num_arms))\n",
        "      return\n",
        "    assert (isinstance(pull_time, int) and pull_time >= 1)\n",
        "    assert (self.reward_sequence.all()!=None), \"please create adverserial setting, i.g sequence of rewards\"\n",
        "    self.total_pull += pull_time #check THIS\n",
        "    # Generate reward\n",
        "    reward = self.reward_sequence[time_step, arm_id] #check THIS; trying multi array indexing for multiple pulls in case of greedy action\n",
        "    self.rewards_history.append(reward)\n",
        "    return reward\n",
        "\n",
        "  def genie_reward(self):\n",
        "    '''\n",
        "    the best expected reward after pulling self.total_pull times\n",
        "    '''\n",
        "    best_reward = sum(np.max(self.reward_sequence[0:self.total_pull,], axis=1))\n",
        "    return best_reward\n",
        "\n",
        "  def my_rewards(self):\n",
        "    return sum(self.rewards_history)\n",
        "\n",
        "  def clear_reward_hist(self):\n",
        "    self.rewards_history = []\n",
        "    self.total_pull = 0\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kGrYTGahGC2",
        "outputId": "2f394e13-7b7b-49cc-ab7d-6418937e3d58"
      },
      "source": [
        "##quick test to check arm works as expected \n",
        "NUM_ARMS = 2\n",
        "my_arm = Adverserial_Arm(num_arms = NUM_ARMS)\n",
        "my_arm.create_reward_seqeunce(N=5)\n",
        "print(my_arm.reward_sequence)\n",
        "\n",
        "my_arm.pull_arm(time_step=0,arm_id=1)\n",
        "my_arm.pull_arm(time_step=2,arm_id=1)\n",
        "\n",
        "print(\"I recieved: \", my_arm.my_rewards())\n",
        "print(\"genie recieved: \", my_arm.genie_reward())\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 0.]\n",
            " [1. 1.]]\n",
            "I recieved:  2.0\n",
            "genie recieved:  2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55VC5w7bhqsK"
      },
      "source": [
        "\n",
        "## Goal of these exercises\n",
        "\n",
        "Implement the following:\n",
        "\n",
        "1. Basic EXP3 algorithm implementation under the Bernoulli bandit setting.\n",
        "2. Plot the expected regret of EXP3 versus horizon (number of rounds).\n",
        "\n",
        "Optional:\n",
        "\n",
        "1. Plot the expected regret of EXP3 versus the number of arms.\n",
        "2. Implement an adversarial bandit, and test EXP3 algorithm on it. \n",
        "\n",
        "## Tips:\n",
        "\n",
        "1. To see if the regret is correct, try to run your EXP3 algorithm repeatedly with horizon equals to $[50^2, 60^2, 70^2, 80^2, 90^2, 100^2]$. Plot your regret (as the y-axis), versus $[50, 60, 70, 80, 90, 100]$ (as the x-axis). The figure should look like a straight line.\n",
        "2. Check out numpy.random.choice for drawing from a discrete distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyJ-_nAAtjrR"
      },
      "source": [
        "\n",
        "NUM_ARMS = 10\n",
        "\n",
        "# class Empirical_mean:\n",
        "#     def __init__(self,num_arms):\n",
        "#         self.rewards = np.zeros(num_arms)\n",
        "#         self.count = np.zeros(num_arms)\n",
        "#         self.mean =  np.zeros(num_arms)\n",
        "#     def add_new_reward(self, reward, indx):\n",
        "#         self.rewards[indx] += reward\n",
        "#         self.count[indx] += 1\n",
        "#         self.mean[indx] = self.rewards[indx] / self.count[indx]\n",
        "#     def mean(self, indx):\n",
        "#         return self.mean[indx]\n",
        "#     def reset(self):\n",
        "#         self.rewards = self.rewards * 0 \n",
        "#         self.count = self.rewards * 0 \n",
        "#         self.mean =  self.rewards * 0 \n",
        "\n",
        "\n",
        "\n",
        "def exp3(arm, N, num_arms=NUM_ARMS, n_rate=0.01):\n",
        "  S = np.zeros(num_arms)\n",
        "  for i in range(N): #verify \n",
        "      prob = np.exp(n_rate*S)/sum(np.exp(n_rate*S)) #calculate sampling distribution \n",
        "      action = int(np.random.choice(len(prob), 1, p = prob)) #here we sample\n",
        "      reward = arm.pull_arm(time_step=i, arm_id=action)\n",
        "      S += 1 ##add 1 to every arm\n",
        "      S[action] -= (1 - reward) * prob[action] ##subtract on arm where indicator is 1\n",
        "  return arm.my_rewards()\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNT9BPMStlNO"
      },
      "source": [
        "\n",
        "def regret_vs_horizon(Ns:list, REPEAT:int, algorithm: type(lambda x: None)):\n",
        "  regret = []\n",
        "  # mu = [0.1, 0.0]\n",
        "  my_arm = Adverserial_Arm(NUM_ARMS)\n",
        "  for NUM_RUNs in Ns:\n",
        "    print(NUM_RUNs)\n",
        "    my_arm.create_reward_seqeunce(N=NUM_RUNs) ## verify; we are creating sequence for each run \n",
        "    cur_regret = 0\n",
        "    for repeat in range(REPEAT):\n",
        "        rewards = algorithm(my_arm, NUM_RUNs, my_arm.num_arms) ## everyrun NUM_RUNs += 10000\n",
        "        cur_regret += my_arm.genie_reward() - rewards\n",
        "        my_arm.clear_reward_hist()\n",
        "    cur_regret /= REPEAT\n",
        "    regret.append(cur_regret)\n",
        "    #\n",
        "  return regret\n",
        "\n",
        "\n",
        "import plotly.graph_objects as graph\n",
        "def plot(all_history:list, title:str, log = False):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        all_history: list of dicts to plot\n",
        "    ret:\n",
        "        None: show plotly fig\n",
        "    \"\"\"\n",
        "    fig = graph.Figure(layout = graph.Layout(title=graph.layout.Title(text=title))) \n",
        "    for i in range(len(all_history)):\n",
        "        fig.add_trace(graph.Scatter(x = all_history[i][\"x\"], \n",
        "                                    y = all_history[i][\"y\"],\n",
        "                                    name = all_history[i][\"legend\"])) \n",
        "    if log: fig.update_xaxes(type=\"log\")\n",
        "    fig.show()\n",
        "\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MprJrV-EtmZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd51ae43-a411-4e14-f9c3-427fcfeec85f"
      },
      "source": [
        "\n",
        "x_axis  = [50,60,70,80,90,100]\n",
        "Ns = np.power(x_axis,2)\n",
        "\n",
        "exp3 = regret_vs_horizon(Ns, REPEAT=100, algorithm=exp3)\n",
        "\n",
        "\n",
        "plot_exp3 = {\"legend\": \"mean_exp3_regret\", \n",
        "                   \"x\": x_axis , \n",
        "                   \"y\": exp3}\n",
        "\n",
        "plot([plot_exp3], title=\"regret VS horizon - linear\" , log = False)\n",
        "# plot([plot_exp3], title=\"regret VS horizon - Log\" , log = True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2500\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}