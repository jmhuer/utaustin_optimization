{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exp3_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/utaustin_optimization/blob/main/homework11/Exp3_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Be43B9Zay2l"
      },
      "source": [
        "# Exponential Weight Algorithm for Explore and Exploit (EXP3)\n",
        "\n",
        "In this exercise, we will be studying the exponential weight algorithm for explore and exploit (EXP3).\n",
        "\n",
        "While this algorithm is designed for adversarial bandit setting, we will test this algorithm in the Bernoulli bandit setting, for the ease of implementation.\n",
        "\n",
        "As an example of the Bernoulli bandit, the following codes create a rewards history matrix $rewards$, such that $rewards[t, a]$ returns the reward you will get if you query arm $a$ at time $t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d80N_Ri-iFWF"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "numAction = 10\n",
        "numRound = 100000\n",
        "expected_rewards = np.arange(numAction) + 1\n",
        "expected_rewards = 1 / expected_rewards\n",
        "expected_rewards = np.repeat(expected_rewards.reshape(1,-1), numRound, axis=0)\n",
        "rewards = np.random.rand(numRound, numAction) < expected_rewards\n",
        "rewards = rewards.astype(float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYtpMYxFs59D"
      },
      "source": [
        "import numpy as np\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Gaussian_Arm:\n",
        "  def __init__(self, num_arms, mu=None):\n",
        "    '''\n",
        "    num_arms: (int). the number of arms\n",
        "    mu: (None or list-type). the mean of the reward of each arm.\n",
        "        if set to None, a random vector will be generated.\n",
        "    '''\n",
        "    if num_arms <= 1 or not isinstance(num_arms, int):\n",
        "      print('number of arms has an int that is at least two')\n",
        "      return\n",
        "    \n",
        "    self.num_arms = num_arms\n",
        "    #\n",
        "    if mu:\n",
        "      self.mu = np.asarray(mu)\n",
        "      if len(self.mu) != num_arms:\n",
        "        print('The lenth of mu does not match the number of arms')\n",
        "        return\n",
        "    else:\n",
        "      self.mu = np.random.rand((num_arms))\n",
        "    # \n",
        "    self.delta = max(self.mu) - min(self.mu)\n",
        "    #\n",
        "\n",
        "    # keep track of the rewards for the user\n",
        "    self.rewards_history = []\n",
        "    # keep track of how many times the arms have been pulled\n",
        "    self.total_pull = 0 \n",
        "\n",
        "  def pull_arm(self, arm_id=-1, pull_time=1):\n",
        "    if arm_id < 0 or arm_id >= self.num_arms:\n",
        "      print('please specify arm id in the range of 0-%d' % (self.num_arms))\n",
        "      return\n",
        "    assert (isinstance(pull_time, int) and pull_time >= 1)\n",
        "    self.total_pull += pull_time\n",
        "    # Generate reward\n",
        "    reward = self.mu[arm_id] * pull_time + sum(np.random.randn(pull_time))\n",
        "    self.rewards_history.append(reward)\n",
        "    return reward\n",
        "\n",
        "\n",
        "  def genie_reward(self):\n",
        "    '''\n",
        "    the best expected reward after pulling self.total_pull times\n",
        "    '''\n",
        "    best_mu = max(self.mu)\n",
        "    return self.total_pull * best_mu\n",
        "\n",
        "  def my_rewards(self):\n",
        "    return sum(self.rewards_history)\n",
        "\n",
        "  def clear_reward_hist(self):\n",
        "    self.rewards_history = []\n",
        "    self.total_pull = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55VC5w7bhqsK"
      },
      "source": [
        "\n",
        "## Goal of these exercises\n",
        "\n",
        "Implement the following:\n",
        "\n",
        "1. Basic EXP3 algorithm implementation under the Bernoulli bandit setting.\n",
        "2. Plot the expected regret of EXP3 versus horizon (number of rounds).\n",
        "\n",
        "Optional:\n",
        "\n",
        "1. Plot the expected regret of EXP3 versus the number of arms.\n",
        "2. Implement an adversarial bandit, and test EXP3 algorithm on it. \n",
        "\n",
        "## Tips:\n",
        "\n",
        "1. To see if the regret is correct, try to run your EXP3 algorithm repeatedly with horizon equals to $[50^2, 60^2, 70^2, 80^2, 90^2, 100^2]$. Plot your regret (as the y-axis), versus $[50, 60, 70, 80, 90, 100]$ (as the x-axis). The figure should look like a straight line.\n",
        "2. Check out numpy.random.choice for drawing from a discrete distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyJ-_nAAtjrR"
      },
      "source": [
        "\n",
        "  \n",
        "NUM_ARMS = 2\n",
        "\n",
        "class Empirical_mean:\n",
        "    def __init__(self,num_arms):\n",
        "        self.rewards = np.zeros(num_arms)\n",
        "        self.count = np.zeros(num_arms)\n",
        "        self.mean =  np.zeros(num_arms)\n",
        "    def add_new_reward(self, reward, indx):\n",
        "        self.rewards[indx] += reward\n",
        "        self.count[indx] += 1\n",
        "        self.mean[indx] = self.rewards[indx] / self.count[indx]\n",
        "    def mean(self, indx):\n",
        "        return self.mean[indx]\n",
        "    def reset(self):\n",
        "        self.rewards = self.rewards * 0 \n",
        "        self.count = self.rewards * 0 \n",
        "        self.mean =  self.rewards * 0 \n",
        "\n",
        "\n",
        "\n",
        "def exp3(arm, N, num_arms=NUM_ARMS, n_rate=0.01):\n",
        "  # bonus = lambda delta, t: np.sqrt((2*np.log(1/delta))/t) if t > 0 else float('inf')\n",
        "  mean_vals = Empirical_mean(num_arms)\n",
        "  for i in range(N):\n",
        "      prob = np.exp(x)/sum(np.exp(x))\n",
        "      mean_vals.add_new_reward(arm.pull_arm(UCBbest_arm), UCBbest_arm)\n",
        "  return arm.my_rewards()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNT9BPMStlNO"
      },
      "source": [
        "\n",
        "def regret_vs_horizon(Ns:list, REPEAT:int, algorithm: type(lambda x: None)):\n",
        "  regret = []\n",
        "  mu = [0.1, 0.0]\n",
        "  my_arm = Gaussian_Arm(NUM_ARMS, mu=mu)\n",
        "  for NUM_RUNs in Ns:\n",
        "    print(NUM_RUNs)\n",
        "    cur_regret = 0\n",
        "    for repeat in range(REPEAT):\n",
        "        rewards = algorithm(my_arm, NUM_RUNs, my_arm.num_arms) ## everyrun NUM_RUNs += 10000\n",
        "        cur_regret += my_arm.genie_reward() - rewards\n",
        "        my_arm.clear_reward_hist()\n",
        "    cur_regret /= REPEAT\n",
        "    regret.append(cur_regret)\n",
        "    #\n",
        "  return regret\n",
        "\n",
        "\n",
        "import plotly.graph_objects as graph\n",
        "def plot(all_history:list, title:str, log = False):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        all_history: list of dicts to plot\n",
        "    ret:\n",
        "        None: show plotly fig\n",
        "    \"\"\"\n",
        "    fig = graph.Figure(layout = graph.Layout(title=graph.layout.Title(text=title))) \n",
        "    for i in range(len(all_history)):\n",
        "        fig.add_trace(graph.Scatter(x = all_history[i][\"x\"], \n",
        "                                    y = all_history[i][\"y\"],\n",
        "                                    name = all_history[i][\"legend\"])) \n",
        "    if log: fig.update_xaxes(type=\"log\")\n",
        "    fig.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MprJrV-EtmZT"
      },
      "source": [
        "\n",
        "Ninit = 200\n",
        "Ns  = [Ninit * (2**i) for i in range(1, 11)]\n",
        "UCB_regret = regret_vs_horizon(Ns, REPEAT=200, algorithm=UCB)\n",
        "\n",
        "\n",
        "plot_UCB_regret = {\"legend\": \"mean_UBC_regret\", \n",
        "                   \"x\": Ns , \n",
        "                   \"y\": UCB_regret}\n",
        "\n",
        "plot([plot_UCB_regret], title=\"regret VS horizon - linear\" , log = False)\n",
        "plot([plot_UCB_regret], title=\"regret VS horizon - Log\" , log = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}