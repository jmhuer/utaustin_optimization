{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Elin-assignment",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/utaustin_optimization/blob/main/homework9/Elin_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpIjBmUzWW_f"
      },
      "source": [
        "# Elimination\n",
        "\n",
        "In this excercises, we will be playing with the Multi-arm bandit problem with the Elinmination algorithm.\n",
        "\n",
        "## Setup\n",
        "\n",
        "As before, consider unstructural bandit problem. Suppose we have $k$ arms, each with random rewards $p_i = u_i + \\epsilon$ where $\\epsilon$ is draw from i.i.d. standard gaussian. (Note that we only require $\\epsilon$ to be sub-gaussian for the analysis to go through)\n",
        "\n",
        "The following codes is capturing the setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E5387xdWXl7"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "\n",
        "class Gaussian_Arm:\n",
        "  def __init__(self, num_arms, mu=None):\n",
        "    '''\n",
        "    num_arms: (int). the number of arms\n",
        "    mu: (None or list-type). the mean of the reward of each arm.\n",
        "        if set to None, a random vector will be generated.\n",
        "    '''\n",
        "    if num_arms <= 1 or not isinstance(num_arms, int):\n",
        "      print('number of arms has an int that is at least two')\n",
        "      return\n",
        "    \n",
        "    self.num_arms = num_arms\n",
        "    #\n",
        "    if mu:\n",
        "      self.mu = np.asarray(mu)\n",
        "      if len(self.mu) != num_arms:\n",
        "        print('The lenth of mu does not match the number of arms')\n",
        "        return\n",
        "    else:\n",
        "      self.mu = np.random.rand((num_arms))\n",
        "    # \n",
        "    self.delta = max(self.mu) - min(self.mu)\n",
        "    #\n",
        "\n",
        "    # keep track of the rewards for the user\n",
        "    self.rewards_history = []\n",
        "    # keep track of how many times the arms have been pulled\n",
        "    self.total_pull = 0 \n",
        "\n",
        "  def pull_arm(self, arm_id=-1, pull_time=1):\n",
        "    if arm_id < 0 or arm_id >= self.num_arms:\n",
        "      print('please specify arm id in the range of 0-%d' % (self.num_arms))\n",
        "      return\n",
        "    assert (isinstance(pull_time, int) and pull_time >= 1)\n",
        "    self.total_pull += pull_time\n",
        "    # Generate reward\n",
        "    reward = self.mu[arm_id] * pull_time + sum(np.random.randn(pull_time))\n",
        "    self.rewards_history.append(reward)\n",
        "    return reward\n",
        "\n",
        "\n",
        "  def genie_reward(self):\n",
        "    '''\n",
        "    the best expected reward after pulling self.total_pull times\n",
        "    '''\n",
        "    best_mu = max(self.mu)\n",
        "    return self.total_pull * best_mu\n",
        "\n",
        "  def my_rewards(self):\n",
        "    return sum(self.rewards_history)\n",
        "\n",
        "  def clear_reward_hist(self):\n",
        "    self.rewards_history = []\n",
        "    self.total_pull = 0"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV8CUnF2WgUD"
      },
      "source": [
        "\n",
        "\n",
        "## Goal of this exercise\n",
        "1. Basic Elimination algorithm implementation\n",
        "2. Plot the regret VS horizon ($n$).\n",
        "3. Implement the doubling trick\n",
        "4. Plot the regret of the doubling trick VS horizon.\n",
        "\n",
        "Please refer to previous assignment for tips.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o3-8o83W9CW"
      },
      "source": [
        "\n",
        "\n",
        "NUM_ARMS = 2\n",
        "\n",
        "def ETC(arm, N, delta, m=None, num_arms=NUM_ARMS):\n",
        "  '''\n",
        "  arm: Arm object.\n",
        "  N: the ultimate time horizon\n",
        "  delta: separation\n",
        "  m: int. The exploration time for EACH arm\n",
        "  num_arms: int. The total number of arms\n",
        "  '''\n",
        "  if m == None:\n",
        "    m = max(1, 4/delta**2*np.log(N * delta**2 / 4))\n",
        "    m = int(m)\n",
        "  assert(m > 0 and isinstance(m, int))\n",
        "  reward_est = np.zeros(num_arms)\n",
        "  for i in range(m):\n",
        "    for j in range(num_arms):\n",
        "      reward_est[j] += arm.pull_arm(j) ##add rewards per arm\n",
        "  best_arm = np.argmax(reward_est)\n",
        "  rewards = sum(reward_est) #sum reward from exploration phase\n",
        "  rewards += arm.pull_arm(best_arm, pull_time=int(N - m*num_arms)) ## pull arm N - m*num_arms\n",
        "  return rewards\n",
        "\n",
        "\n",
        "def doubling_ETC(arm, N, delta, num_arms=NUM_ARMS):\n",
        "  total_run = 0\n",
        "  r = 8\n",
        "  #\n",
        "  while total_run < N:\n",
        "    r += 1\n",
        "    N_r = num_arms ** r\n",
        "    if total_run + N_r > N:\n",
        "      N_r = N - total_run\n",
        "    m_r = max(1, 4/delta**2*np.log(N_r * delta**2 / 4))\n",
        "    m_r = int(m_r)\n",
        "    ETC(arm, N_r, delta)\n",
        "    total_run += N_r\n",
        "  return arm.my_rewards()\n"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svfee6G8XCwM"
      },
      "source": [
        "\n",
        "def regret_vs_horizon(Ns:list):\n",
        "  REPEAT = 1\n",
        "  ETC_regret = []\n",
        "  ETC_doubling_regret = []\n",
        "  mu = [0.1, 0.0]\n",
        "  my_arm = Gaussian_Arm(NUM_ARMS, mu=mu)\n",
        "  for NUM_RUNs in Ns:\n",
        "    # print(NUM_RUNs)\n",
        "    ETC_cur_regret = 0\n",
        "    ETC_doubling_cur_regret = 0\n",
        "    for repeat in range(REPEAT):\n",
        "      ETC_rewards = ETC(my_arm, NUM_RUNs, my_arm.delta) ## everyrun NUM_RUNs += 10000\n",
        "      ETC_cur_regret += my_arm.genie_reward() - ETC_rewards\n",
        "      my_arm.clear_reward_hist()\n",
        "      #\n",
        "      ETC_doubling_rewards = doubling_ETC(my_arm, NUM_RUNs, my_arm.delta)\n",
        "      ETC_doubling_cur_regret += my_arm.genie_reward() - ETC_doubling_rewards\n",
        "      my_arm.clear_reward_hist()\n",
        "    ETC_cur_regret /= REPEAT\n",
        "    ETC_doubling_cur_regret /= REPEAT\n",
        "    ETC_regret.append(ETC_cur_regret)\n",
        "    ETC_doubling_regret.append(ETC_doubling_cur_regret)\n",
        "    #\n",
        "  return ETC_regret, ETC_doubling_regret\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V14C4j-o1qbu",
        "outputId": "d39657b3-251d-4fbb-b302-849ea7344b6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "Ninit = 10000\n",
        "Ns  = [Ninit * i for i in range(1, 20)]\n",
        "avg = 5\n",
        "mean_ETC_regret, mean_ETC_doubling_regret = np.zeros((avg , len(Ns))), np.zeros((avg , len(Ns))) ##initialize mean arrays\n",
        "for i in range(avg):\n",
        "  print(\"simulation {} completed\".format(i))\n",
        "  ETC_regret,  ETC_doubling_regret = regret_vs_horizon(Ns)\n",
        "  mean_ETC_regret[i] = ETC_regret\n",
        "  mean_ETC_doubling_regret[i] = ETC_doubling_regret\n",
        "\n",
        "\n",
        "# '''\n",
        "# just in case you don't want to spend 2 - 4 CPU hours on it:\n",
        "# ETC_regret = [131.82144761, 164.05276726, 178.14454248, 186.96920924,\n",
        "#        200.71992072, 211.24218225, 209.97037274, 225.29555665,\n",
        "#        226.31542287, 227.84898093, 233.52895855, 231.61707574,\n",
        "#        232.73323221, 230.47352683, 245.42566151, 246.2055098 ,\n",
        "#        244.76404079, 249.82165257, 247.08038319]\n",
        "# ETC_doubling_regret = [ 296.28143055,  441.84768859,  491.78999568,  625.13853049,\n",
        "#         655.64274781,  675.67153638,  789.59309794,  829.8073726 ,\n",
        "#         855.97071018,  864.50042971,  872.11629734,  884.49669962,\n",
        "#         878.11998828, 1012.95650198, 1048.23083199, 1062.88694778,\n",
        "#        1071.05337232, 1078.68582528, 1099.49708922]\n",
        "# '''"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "simulation 0 completed\n",
            "simulation 1 completed\n",
            "simulation 2 completed\n",
            "simulation 3 completed\n",
            "simulation 4 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu7rC0e2Xwkv"
      },
      "source": [
        "\n",
        "def plot_history(all_history:dict, x:str, y:str , title:str , log = False):\n",
        "  fig = graph.Figure(layout = graph.Layout(title=graph.layout.Title(text=title))) \n",
        "  for i in all_history:\n",
        "    fig.add_trace(graph.Scatter(x = all_history[i][x], y = all_history[i][y],name = i)) \n",
        "    if log: fig.update_xaxes(type=\"log\")\n",
        "  fig.show()\n",
        "plot_history(all_history, \"x\", \"y\" , \"regret VS horizon - linear\" , log = False)\n",
        "\n",
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "# import matplotlib\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.plot(runs, ETC_regret, label='ETC')\n",
        "# plt.title('regret VS horizon - linear')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "# plt.plot(runs, ETC_doubling_regret, label='ETC doubling')\n",
        "# plt.title('regret VS horizon - linear')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejpiFNd81OIS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S89af3tSqchw"
      },
      "source": [
        "runs = Ns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.semilogx(runs, ETC_regret, label='ETC')\n",
        "plt.title('regret VS horizon - semilogx')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.semilogx(runs, ETC_doubling_regret, label='ETC doubling')\n",
        "plt.title('regret VS horizon - semilogx')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}